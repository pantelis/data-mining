

<!DOCTYPE html>


<html lang="en" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>Simple RNN &#8212; Data Mining</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../../../_static/styles/theme.css?digest=ac02cc09edc035673794" rel="stylesheet" />
<link href="../../../../_static/styles/bootstrap.css?digest=ac02cc09edc035673794" rel="stylesheet" />
<link href="../../../../_static/styles/pydata-sphinx-theme.css?digest=ac02cc09edc035673794" rel="stylesheet" />

  
  <link href="../../../../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=ac02cc09edc035673794" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../../../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../../../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../../../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../../../_static/pygments.css" />
    <link rel="stylesheet" href="../../../../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../../../_static/scripts/bootstrap.js?digest=ac02cc09edc035673794" />
<link rel="preload" as="script" href="../../../../_static/scripts/pydata-sphinx-theme.js?digest=ac02cc09edc035673794" />
  <script src="../../../../_static/vendor/fontawesome/6.1.2/js/all.min.js?digest=ac02cc09edc035673794"></script>

    <script data-url_root="../../../../" id="documentation_options" src="../../../../_static/documentation_options.js"></script>
    <script src="../../../../_static/jquery.js"></script>
    <script src="../../../../_static/underscore.js"></script>
    <script src="../../../../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../../../../_static/doctools.js"></script>
    <script src="../../../../_static/clipboard.min.js"></script>
    <script src="../../../../_static/copybutton.js"></script>
    <script src="../../../../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../../../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../../../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../../../../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"tex": {"macros": {"floor": ["\\lfloor#1\\rfloor", 1], "bmat": ["\\left[\\begin{array}"], "emat": ["\\end{array}\\right]"]}}, "options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'aiml-common/lectures/rnn/simple-rnn/_index';</script>
    <link rel="canonical" href="https://pantelis.github.io/data-mining/aiml-common/lectures/rnn/simple-rnn/_index.html" />
    <link rel="shortcut icon" href="../../../../_static/logo.ico"/>
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" />
    <link rel="next" title="The Long Short-Term Memory (LSTM) Architecture" href="../lstm/_index.html" />
    <link rel="prev" title="Introduction to Recurrent Neural Networks (RNN)" href="../introduction/_index.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>

  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
  

<a class="navbar-brand logo" href="../../../../intro.html">
  
  
  
  
    
    
      
    
    
    <img src="../../../../_static/logo.png" class="logo__image only-light" alt="Logo image"/>
    <script>document.write(`<img src="../../../../_static/logo.png" class="logo__image only-dark" alt="Logo image"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../../../intro.html">
                    Data Mining
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Syllabus</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../../../syllabus/index.html">Syllabus</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Introduction to Data Mining</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../data-premise/index.html">The new premise</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../ai-intro/data-science-360/_index.html">Data Science 360</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../pipelines/_index.html">Pipelines</a><input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-1"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../pipelines/uber-ml-arch-case-study/index.html">A Case Study of an ML Architecture - Uber</a></li>

<li class="toctree-l2"><a class="reference internal" href="../../pipelines/01_the_machine_learning_landscape.html">The Machine Learning landscape</a></li>



</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">The Learning Problem</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../learning-problem/_index.html">The Learning Problem</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../pipelines/02_end_to_end_machine_learning_project.html">End-to-end Machine Learning project</a></li>






<li class="toctree-l1"><a class="reference internal" href="../../model-selection/bias_variance.html">Empirical Risk Minimization</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Predictors for Structured Data</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../entropy/_index.html">Entropy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../trees/decision-trees/index.html">Decision Trees</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../trees/decision-trees/decision_tree.html">Decision tree from scratch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../trees/decision-trees/decision_tree_visualisations.html">Visualizing tree-based classifiers</a></li>

<li class="toctree-l1"><a class="reference internal" href="../../trees/regression-trees/regression_tree_visualisations.html">Visualizing tree-based regressors</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../ensemble/index.html">Ensemble Methods</a><input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-2"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../ensemble/random-forests/index.html">Random Forests</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../ensemble/adaboost/index.html">Adaptive Boosting (AdaBoost)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../ensemble/adaboost/adaboost_example.html">Adaboost from scratch</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../ensemble/gradient-boosting/index.html">Gradient Boosting</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../ensemble/boosting-workshop/index.html">Boosting Workshop</a></li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Learning without Labels or Without Parameters</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../unsupervised/k-means/_index.html">K-means Clustering</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../density-estimation/knn/_index.html">k-Nearest Neighbors (kNN) Classification</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../density-estimation/knn-workshop/_index.html">kNN Workshop</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Dimensionality Reduction</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../pca/introduction/index.html">Principal Component Analysis (PCA)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../pca/introduction/principal_component_analysis.html">PCA Workshop</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../pca/whitening/index.html">Data Preprocessing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../pca/whitening/preprocessing_workshop.html">Preprocessing: from covariance matrix to image whitening</a></li>




<li class="toctree-l1"><a class="reference internal" href="../../recommenders/recommenders-intro/_index.html">Introduction to Recommender Systems</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../recommenders/netflix/_index.html">The Netflix Prize and Singular Value Decomposition</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Regression and Classification</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../regression/linear-regression/linear_regression.html">Linear Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../optimization/sgd/_index.html">Stochastic Gradient Descent</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../classification/classification-intro/_index.html">Introduction to Classification</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../classification/logistic-regression/_index.html">Logistic Regression</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Deep Neural Networks</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../classification/perceptron/_index.html">The Neuron (Perceptron)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../dnn/dnn-intro/_index.html">Deep Neural Networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../dnn/fashion-mnist-case-study.html">Fashion MNIST Case Study</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../optimization/regularization/_index.html">Regularization in Deep Neural Networks</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Convolutional Neural Networks</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../cnn/cnn-intro/_index.html">Introduction to Convolutional Neural Networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../cnn/cnn-layers/_index.html">CNN Architectures</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../cnn/cnn-example-architectures/_index.html">CNN Example Architectures</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../cnn/cnn-example-architectures/using_convnets_with_small_datasets.html">Using convnets with small datasets</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../scene-understanding/feature-extraction-resnet/index.html">Feature Extraction via Residual Networks</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Representation Learning and Autoencoders</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../vae/index.html">Variational Auto Encoders (VAE)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../vae/generative-modeling/index.html">Generative Modeling and Approximate Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../vae/vae-architecture/index.html">VAE Architecture</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../vae/elbo-optimization/index.html">ELBO Optimization</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Sequences and RNNs</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../introduction/_index.html">Introduction to Recurrent Neural Networks (RNN)</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Simple RNN</a></li>
<li class="toctree-l1"><a class="reference internal" href="../lstm/_index.html">The Long Short-Term Memory (LSTM) Architecture</a></li>
<li class="toctree-l1"><a class="reference internal" href="../time_series_using_simple_rnn_lstm.html">Time Series Prediction using RNNs</a></li>





</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Natural Language Processing</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../nlp/nlp-introduction/nlp-pipelines/_index.html">Introduction to NLP Pipelines</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../nlp/nlp-introduction/tokenization/index.html">Tokenization</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../nlp/nlp-introduction/word2vec/_index.html">Embeddings</a><input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-3"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../nlp/nlp-introduction/word2vec/word2vec_from_scratch.html">Word2Vec from scratch</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../nlp/nlp-introduction/word2vec/word2vec_tensorflow_tutorial.html">Word2Vec Tensorflow Tutorial</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../nlp/language-models/_index.html">Language Models</a><input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-4"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../nlp/language-models/cnn-language-model/index.html">CNN Language Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../nlp/language-models/simple-rnn-language-model/index.html">Simple RNN Language Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../nlp/language-models/lstm-language-model/index.html">LSTM Language Model from scratch</a></li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Math Background</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../ml-math/index.html">Math for ML Textbook</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../ml-math/probability/index.html">Probability Basics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../ml-math/linear-algebra/index.html">Linear Algebra for Machine Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../ml-math/calculus/index.html">Calculus</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Resources</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../../resources/environment/index.html">Your Programming Environment</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../resources/environment/assignment-submission.html">Submitting Your Assignment / Project</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../python/index.html">Learn Python</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../resources/environment/notebook-status.html">Notebook execution status</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Assignments</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../../assignments/probability/probability-assignment-7/index.html">Probability Assignment</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../assignments/ensemble/ais/index.html">Classifying Ships from Automatic Indentification System (AIS) Data</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/pantelis/data-mining" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/pantelis/data-mining/edit/master/data_mining/aiml-common/lectures/rnn/simple-rnn/_index.md" target="_blank"
   class="btn btn-sm btn-source-edit-button dropdown-item"
   title="Suggest edit"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>
<span class="btn__text-container">Suggest edit</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/pantelis/data-mining/issues/new?title=Issue%20on%20page%20%2Faiml-common/lectures/rnn/simple-rnn/_index.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../../../../_sources/aiml-common/lectures/rnn/simple-rnn/_index.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>

<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Simple RNN</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#dimensioning-simple-rnns">Dimensioning Simple RNNs</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#forward-propagation">Forward Propagation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#back-propagation-through-time-bptt">Back-Propagation Through Time (BPTT)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#vanishing-or-exploding-gradients">Vanishing or exploding gradients</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="simple-rnn">
<h1>Simple RNN<a class="headerlink" href="#simple-rnn" title="Permalink to this heading">#</a></h1>
<p>The simple RNN architecture with just a single layer of neurons that receive the input <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> is shown below.</p>
<p><img alt="simple-rnn-single-layer" src="../../../../_images/simple-rnn-simple-layer.png" />
<em>Simple RNN layer with <span class="math notranslate nohighlight">\(n_{neurons}\)</span> RNN neurones mapping the input to the hidden-state at each time step.</em></p>
<p>A more practical simple RNN architecture is shown below.</p>
<p><img alt="rnn-hidden-recurrence" src="../../../../_images/rnn-hidden-recurrence.png" /></p>
<p><em>Simple RNN with recurrences between hidden units. This architecture can compute any computable function and therefore is a <a class="reference external" href="http://alvyray.com/CreativeCommons/BizCardUniversalTuringMachine_v2.3.pdf">Universal Turing Machine</a>.</em></p>
<p>Notice how the path from input <span class="math notranslate nohighlight">\(\mathbf x_{t-1}\)</span> affects the label <span class="math notranslate nohighlight">\(\mathbf y_{t}\)</span> and also the conditional independence between <span class="math notranslate nohighlight">\(\mathbf y\)</span> given <span class="math notranslate nohighlight">\(\mathbf x\)</span>.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Please note that to use this diagram for backpropagation, you must flip the roles of nodes and edges shown. Tensors must be on edges and RNN functions (gates) must be in  the nodes of the graph.</p>
</div>
<section id="dimensioning-simple-rnns">
<h2>Dimensioning Simple RNNs<a class="headerlink" href="#dimensioning-simple-rnns" title="Permalink to this heading">#</a></h2>
<p>In the table below <span class="math notranslate nohighlight">\(m\)</span> is the number of examples in the mini-batch.</p>
<table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Variable</p></th>
<th class="head"><p>Dimensions</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(\mathbf{h}_t\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(n_{neurons} \times 1\)</span> or <span class="math notranslate nohighlight">\(m \times n_{neurons}\)</span></p></td>
</tr>
<tr class="row-odd"><td><p><span class="math notranslate nohighlight">\(\mathbf{x}_t\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(n_{input} \times 1\)</span> or <span class="math notranslate nohighlight">\(m \times n_{input}\)</span></p></td>
</tr>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(\mathbf{U}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(n_{neurons} \times n_{input}\)</span></p></td>
</tr>
<tr class="row-odd"><td><p><span class="math notranslate nohighlight">\(\mathbf{W}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(n_{neurons} \times n_{neurons}\)</span></p></td>
</tr>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(\mathbf{b}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(n_{neurons} \times 1\)</span></p></td>
</tr>
<tr class="row-odd"><td><p><span class="math notranslate nohighlight">\(\mathbf{V}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(n_{output} \times n_{neurons}\)</span></p></td>
</tr>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(\mathbf{o}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(n_{output} \times 1\)</span></p></td>
</tr>
<tr class="row-odd"><td><p><span class="math notranslate nohighlight">\(\mathbf{c}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(n_{output} \times 1\)</span></p></td>
</tr>
</tbody>
</table>
<p>Please note that there may be multiple layers that can be stacked on top of each other and they can individually keep a hidden state.</p>
</section>
<section id="forward-propagation">
<h2>Forward Propagation<a class="headerlink" href="#forward-propagation" title="Permalink to this heading">#</a></h2>
<p>This network maps the input sequence to a sequence of the same length and implements the following forward pass:</p>
<div class="math notranslate nohighlight">
\[\mathbf a_t = \mathbf W \mathbf h _{t-1} + \mathbf U \mathbf x_t + \mathbf b\]</div>
<div class="math notranslate nohighlight">
\[\mathbf h_t = \tanh(\mathbf a_t)\]</div>
<div class="math notranslate nohighlight">
\[\mathbf o_t = \mathbf V \mathbf h_t + \mathbf c\]</div>
<div class="math notranslate nohighlight">
\[\mathbf{\hat y_t} = \mathtt{softmax}(\mathbf o_t)\]</div>
<div class="math notranslate nohighlight">
\[L(\hat{\mathbf y}_1, \dots , \hat{\mathbf y}_{\tau}, \mathbf y_1, \dots , \mathbf y_{\tau}) = D_{KL}[\hat p_{data}(\mathbf y | \mathbf x) || p_{model}(\mathbf y | \mathbf x; \mathbf \theta)]\]</div>
<div class="math notranslate nohighlight">
\[= - \mathbb E_{\mathbf y | \mathbf x \sim \hat{p}_{data}} \log p_{model}(\mathbf y | \mathbf x ; \mathbf \theta)  = - \sum_t \log p_{model}(y_t | \mathbf x_1, \dots, \mathbf x_t ; \mathbf \theta)\]</div>
<p>Notice that RNNs can model very generic distributions  <span class="math notranslate nohighlight">\(\log p_{model}(\mathbf x, \mathbf y ; \mathbf \theta)\)</span>. The simple RNN architecture above, effectively models the posterior distribution <span class="math notranslate nohighlight">\(\log p_{model}(\mathbf y | \mathbf x ; \mathbf \theta)\)</span>  and based on a conditional independence assumption it factorizes into <span class="math notranslate nohighlight">\(\sum_t \log p_{model}(y_t | \mathbf x_1, \dots, \mathbf x_t ; \mathbf \theta)\)</span>.</p>
<div class="admonition-note admonition">
<p class="admonition-title">Note</p>
<p>Altough we dont expand further here, note that by connecting the <span class="math notranslate nohighlight">\(\mathbf y_{t-1}\)</span> to <span class="math notranslate nohighlight">\(\mathbf h_t\)</span> via a matrix e.g. <span class="math notranslate nohighlight">\(\mathbf R\)</span> we can avoid this simplifying assumption and be able to model an arbitrary distribution <span class="math notranslate nohighlight">\(\log p_{model}(\mathbf y | \mathbf x ; \mathbf \theta)\)</span>. In other words just like in the other DNN architectures, connectivity directly affects the representational capacity of the hypothesis set.</p>
</div>
<p>In many instances we have problems where it only matters the label <span class="math notranslate nohighlight">\(y_\tau\)</span> at the end of the sequence. Lets say that you are classifying speech or video inside the cabin of a car to detect the psychological state of the driver. The same architecture shown above can also represent such problems - the only difference is the only the <span class="math notranslate nohighlight">\(\mathbf o_\tau\)</span>, <span class="math notranslate nohighlight">\(L_\tau\)</span> and <span class="math notranslate nohighlight">\(y_\tau\)</span> will be considered.</p>
<p>Lets see an example to understand better the forward propagation equations.</p>
<p><img alt="example-sentence" src="../../../../_images/example-sentence.png" />
<em>Example sentence as input to the RNN</em></p>
<p>In the figure above you have a hypothetical document (a sentence) that is broken into <em>tokens</em> - lets say that a token is a word in this case. In the simpler case where we need a classification of the whole document, given that <span class="math notranslate nohighlight">\(\tau=6\)</span>, we are going to receive at t=1, the first token <span class="math notranslate nohighlight">\(\mathbf x_1\)</span> and with an input hidden state  <span class="math notranslate nohighlight">\(\mathbf h_0 = 0\)</span> we will calculate the forward equations for <span class="math notranslate nohighlight">\(\mathbf h_1\)</span>, ignoring the output <span class="math notranslate nohighlight">\(\mathbf o_1\)</span> and repeat the unrolling when the next input <span class="math notranslate nohighlight">\(\mathbf x_2\)</span> comes in until we reach the end of sentence token <span class="math notranslate nohighlight">\(\mathbf x_6\)</span> which in this case will calculate the output <span class="math notranslate nohighlight">\(y_6\)</span> and loss</p>
<div class="math notranslate nohighlight">
\[- \log p_{model} (y_6|\mathbf x_1, \dots , \mathbf x_6; \mathbf  w)\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathbf \theta = \{ \mathbf W, \mathbf U, \mathbf V, \mathbf b, \mathbf c \}\)</span>.</p>
</section>
<section id="back-propagation-through-time-bptt">
<h2>Back-Propagation Through Time (BPTT)<a class="headerlink" href="#back-propagation-through-time-bptt" title="Permalink to this heading">#</a></h2>
<p>Lets now see how the training through backward propagation would work for RNNs.</p>
<p><img alt="rnn-BPTT" src="../../../../_images/rnn-BPTT.png" />
<em>Understanding RNN memory through BPTT procedure</em></p>
<p>Backpropagation is similar to that of other neural networks simply because the unrolled architecture resembles a feed forward arrangement. But there is an important difference and we can see it using the above computational graph for the unrolled recurrences <span class="math notranslate nohighlight">\(t\)</span> and <span class="math notranslate nohighlight">\(t-1\)</span>. During computation of the variable <span class="math notranslate nohighlight">\(\mathbf h_t\)</span> we use the value of the variable <span class="math notranslate nohighlight">\(\mathbf h_{t-1}\)</span> calculated in the previous recurrence. So when we apply the chain rule in the backward phase of BP, for all nodes that involve such variables with recurrent dependencies, the end result is that <em>non local</em> gradients appear from previous backpropagation steps (<span class="math notranslate nohighlight">\(t\)</span> in the figure). This is in contrast to other networks where during BP only local to each gate gradients where involved as we have seen earlier.</p>
<p>This is effectively why we say that simple RNNs <strong>feature <em>memory</em></strong>.  The key point to notice in the backpropagation in recurrence <span class="math notranslate nohighlight">\(t-1\)</span> is the junction between <span class="math notranslate nohighlight">\(\tanh\)</span> and <span class="math notranslate nohighlight">\(\mathbf V \mathbf h_{t-1}\)</span>. This junction brings in the gradient <span class="math notranslate nohighlight">\(\nabla_{\mathbf h_{t-1}}L_t\)</span> from the backpropagation of the <span class="math notranslate nohighlight">\(\mathbf W h_{t-1}\)</span> node in recurrence <span class="math notranslate nohighlight">\(t\)</span> and just because its a junction, it is added to the backpropagated gradient from above in the current recurrence <span class="math notranslate nohighlight">\(t-1\)</span> i.e.</p>
<div class="math notranslate nohighlight">
\[\nabla_{\mathbf h_{t-1}}L_{t-1} \leftarrow \nabla_{\mathbf h_{t-1}}L_{t-1} + \nabla_{\mathbf h_{t-1}}L_t \]</div>
<p><a class="reference external" href="https://www.deeplearningbook.org/contents/rnn.html">Ian Goodfellow’s section 10.2.2</a> provides the exact equations - please note that you need to know only the intuition behind computational graphs for RNNs. In practice, BPTT is [<em>truncated</em>](<a class="reference external" href="https://proceedings.mlr.press/v115/aicher20a.html#:~:text=Truncated%20backpropagation%20through%20time%20(TBPTT,a%20fixed%20number%20of%20lags.)">https://proceedings.mlr.press/v115/aicher20a.html#:~:text=Truncated backpropagation through time (TBPTT,a fixed number of lags.)</a> to avoid having to do one full forward pass and one full reverse pass through the training dataset of a e.g. language model that is usually very large, to do a single gradient update. When the truncation level is not sufficiently large, the bias introduced can cause SGD to not converge. In practice, a large truncation size is chosen heuristically (e.g. larger than the expected ‘memory’ of the system) or via cross-validation.</p>
</section>
<section id="vanishing-or-exploding-gradients">
<h2>Vanishing or exploding gradients<a class="headerlink" href="#vanishing-or-exploding-gradients" title="Permalink to this heading">#</a></h2>
<div class="admonition-an-iir-filter-analogy admonition">
<p class="admonition-title">An IIR filter analogy</p>
<p>In the figure below we have drafted a conceptual version of what is happening with recurrences over time. Its called an Infinite Impulse Response (IIR) filter for reasons that will be apparent shortly.</p>
<p><img alt="rnn-IIR" src="../../../../_images/rnn-IIR.png" />
<em>Infinite Impulse Response (IIR) filter with weight <span class="math notranslate nohighlight">\(w\)</span></em></p>
<p>With <span class="math notranslate nohighlight">\(D\)</span> denoting a unit delay (a memory location that we can store and retrieve a value), the recurrence formula for this system is:</p>
<div class="math notranslate nohighlight">
\[h_t = w h_{t-1} + x_t\]</div>
<p>where <span class="math notranslate nohighlight">\(w\)</span>is a weight (a scalar). Lets consider what happens when an impulse, <span class="math notranslate nohighlight">\(x_t = \delta_t\)</span> is fed at the input of this system with <span class="math notranslate nohighlight">\(w=-0.9\)</span>.</p>
<div class="math notranslate nohighlight">
\[h_0 = -0.9 h_{-1} + \delta_0 = 1\]</div>
<div class="math notranslate nohighlight">
\[h_1 = -0.9 h_{0} + \delta_1 = -0.9\]</div>
<div class="math notranslate nohighlight">
\[h_2 = -0.9 h_{1} + \delta_2 = 0.81\]</div>
<div class="math notranslate nohighlight">
\[h_3 = -0.9 h_{2} + \delta_3 = -0.729\]</div>
<p>With <span class="math notranslate nohighlight">\(w=-0.9\)</span>, the h_t (called impulse response) follows a decaying exponential envelope while obviously with <span class="math notranslate nohighlight">\(w &gt; 1.0\)</span> it would follow an exponentially increasing envelope. Such recurrences if continue will result in vanishing or exploding responses long after the impulse showed up in the input <span class="math notranslate nohighlight">\(t=0\)</span>.</p>
<p>Using this primitive IIR filter as an analogy, we can see that the weight plays a crucial role in the impulse response.</p>
</div>
<p>In a similar fashion, the RNN hidden state recurrence, in the backwards pass that extends from the <span class="math notranslate nohighlight">\(t=\tau\)</span> to <span class="math notranslate nohighlight">\(t=1\)</span> can make the gradient, when <span class="math notranslate nohighlight">\(\tau\)</span> is large, either <em>vanish</em> or <em>explode</em>. Instead of a scalar <span class="math notranslate nohighlight">\(w\)</span> we saw in the IIR filter we have matrices <span class="math notranslate nohighlight">\(\mathbf W\)</span> and instead of <span class="math notranslate nohighlight">\(h\)</span> we have gradients <span class="math notranslate nohighlight">\(\nabla_{\mathbf h_{t}}L_{t}\)</span>. See <a class="reference external" href="http://proceedings.mlr.press/v28/pascanu13.pdf">this</a> paper for details.</p>
<p>The gradient of the <span class="math notranslate nohighlight">\(\tanh\)</span> non-linearity shown below, is between 0 and 1 suppressing the gradients and slowing down training.</p>
<p><img alt="tanh-derivative" src="../../../../_images/tanh-derivative.png" />
<em>Derivative of <span class="math notranslate nohighlight">\(\tanh\)</span> non-linearity</em></p>
<p>Similar the successive application of the <span class="math notranslate nohighlight">\(W\)</span> matrix is causing explosive gradients as simplistically (ignoring the non-linearity) the hidden state can be written as:</p>
<div class="math notranslate nohighlight">
\[\mathbf h_{t} = \mathbf W \mathbf h_{t-1}\]</div>
<p>making after <span class="math notranslate nohighlight">\(\tau\)</span> steps</p>
<div class="math notranslate nohighlight">
\[\mathbf h_{t} = \mathbf W^\tau \mathbf h_{0}\]</div>
<p>If the magnitude of the eigenvalues are less than 1.0 the matrix will create vanishing gradients as it is involved in the <span class="math notranslate nohighlight">\(\nabla_{\mathbf h_{t}}L_{t}\)</span> expression (see <a class="reference external" href="https://www.deeplearningbook.org/contents/rnn.html">equations in section 10.2.2</a>).  This issue is addressed using an evolved RNN architecture called Long Short Term Memory (LSTM).</p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "pantelis/data-mining",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./aiml-common/lectures/rnn/simple-rnn"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  <!-- Previous / next buttons -->
<div class="prev-next-area">
    <a class="left-prev"
       href="../introduction/_index.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Introduction to Recurrent Neural Networks (RNN)</p>
      </div>
    </a>
    <a class="right-next"
       href="../lstm/_index.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">The Long Short-Term Memory (LSTM) Architecture</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#dimensioning-simple-rnns">Dimensioning Simple RNNs</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#forward-propagation">Forward Propagation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#back-propagation-through-time-bptt">Back-Propagation Through Time (BPTT)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#vanishing-or-exploding-gradients">Vanishing or exploding gradients</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Pantelis Monogioudis, Ph.D
</p>

  </div>
  
  <div class="footer-item">
    
  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../../../_static/scripts/bootstrap.js?digest=ac02cc09edc035673794"></script>
<script src="../../../../_static/scripts/pydata-sphinx-theme.js?digest=ac02cc09edc035673794"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>