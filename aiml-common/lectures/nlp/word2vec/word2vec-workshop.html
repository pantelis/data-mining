
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Word2Vec Workshop &#8212; Data Mining</title>
    
  <link href="../../../../_static/css/theme.css" rel="stylesheet">
  <link href="../../../../_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../../../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../../../../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../../../../_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script data-url_root="../../../../" id="documentation_options" src="../../../../_static/documentation_options.js"></script>
    <script src="../../../../_static/jquery.js"></script>
    <script src="../../../../_static/underscore.js"></script>
    <script src="../../../../_static/doctools.js"></script>
    <script src="../../../../_static/togglebutton.js"></script>
    <script src="../../../../_static/clipboard.min.js"></script>
    <script src="../../../../_static/copybutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../../../_static/sphinx-book-theme.d59cb220de22ca1c485ebbdc042f0030.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script async="async" src="https://unpkg.com/thebe@0.5.1/lib/index.js"></script>
    <script>
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../../../../_static/sphinx-thebe.js"></script>
    <link rel="canonical" href="https://pantelis.github.io/data-mining/aiml-common/lectures/nlp/word2vec/word2vec-workshop.html" />
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../../../../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../../../../_static/logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Data Mining</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../../../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search the docs ..." aria-label="Search the docs ..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Syllabus
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../syllabus/_index.html">
   Syllabus
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Introduction to Data Mining
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../ai-intro/course-introduction/_index.html">
   Course Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../ai-intro/data-science-360/_index.html">
   Data Science 360
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../ai-intro/pipelines/_index.html">
   ML Pipelines
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../uber-ml-arch-case-study/_index.html">
   A Case Study of an ML Architecture - Uber
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  The Learning Problem
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../learning-problem/_index.html">
   The Learning Problem
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../regression/linear-regression/_index.html">
   Linear Regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../optimization/maximum-likelihood/_index.html">
   Maximum Likelihood (ML) Estimation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../entropy/_index.html">
   Entropy
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../optimization/sgd/_index.html">
   Stochastic Gradient Descent
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../classification/classification-intro/_index.html">
   Introduction to Classification
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Classical Learning Methods
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../classification/logistic-regression/_index.html">
   Logistic Regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../decision-trees/_index.html">
   Decision Trees
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../ensemble/_index.html">
   Ensemble Methods
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../ensemble/random-forests/_index.html">
   Random Forests
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../ensemble/boosting/_index.html">
   Boosting
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../ensemble/boosting-workshop/_index.html">
   Boosting workshop
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../pgm/bayesian-inference/_index.html">
   Bayesian Inference
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../pgm/bayesian-coin/_index.html">
   Bayesian Coin Flipping
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../classification/covid19-antibody-test/_index.html">
   COVID-19 Antibody Test
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Deep Neural Networks
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../classification/perceptron/_index.html">
   The Neuron (Perceptron)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../dnn/dnn-intro/_index.html">
   Deep Neural Networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../dnn/backprop-intro/_index.html">
   Introduction to Backpropagation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../dnn/backprop-dnn/_index.html">
   Backpropagation in Deep Neural Networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../optimization/regularization/_index.html">
   Regularization in Deep Neural Networks
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Convolutional Neural Networks
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../cnn/cnn-intro/_index.html">
   Introduction to Convolutional Neural Networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../cnn/cnn-layers/_index.html">
   CNN Architectures
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../cnn/cnn-example-architectures/_index.html">
   CNN Example Architectures
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Scene Understanding
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../scene-understanding/scene-understanding-intro/_index.html">
   Introduction to Scene Understanding
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../scene-understanding/feature-extraction-resnet/_index.html">
   Feature Extraction via Residual Networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../scene-understanding/object-detection/_index.html">
   Object Detection
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../scene-understanding/object-detection/detection-segmentation-workshop/_index.html">
   Object Detection and Semantic Segmentation Workshop
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../cnn/cnn-explainers/_index.html">
   CNN Explainers
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Sequences and RNNs
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../rnn/introduction/_index.html">
   Introduction to Recurrent Neural Networks (RNN)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../rnn/simple-rnn/_index.html">
   Simple RNN
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../rnn/simple-rnn-workshop/_index.html">
   Simple RNN Workshop
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../rnn/lstm/_index.html">
   The Long Short-Term Memory (LSTM) Cell Architecture
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Embeddings and NLP
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../nlp-intro/_index.html">
   Introduction to NLP
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="_index.html">
   Word2Vec Embeddings
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../rnn-language-models/_index.html">
   RNN Language Models
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../nmt/_index.html">
   Neural Machine Translation
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Non-Parametric Methods
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../unsupervised/k-means/_index.html">
   K-means Clustering
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../density-estimation/knn/_index.html">
   k-Nearest Neighbors (kNN) Classification
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../density-estimation/knn-workshop/_index.html">
   kNN Workshop
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Dimensionality Reduction
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../pca/_index.html">
   Principal Component Analysis (PCA)
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Math Background
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../ml-math/probability/_index.html">
   Probability Basics
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../linear-algebra/vectors/_index.html">
   Vectors
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../linear-algebra/matrices/_index.html">
   Matrices
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../ml-math/calculus/_index.html">
   Calculus
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Resources
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../python/_index.html">
   Learn Python
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Theme by the <a href="https://ebp.jupyterbook.org">Executable Book Project</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        
        <a class="issues-button"
            href="https://pantelis.github.io/data-mining/issues/new?title=Issue%20on%20page%20%2Faiml-common/lectures/nlp/word2vec/word2vec-workshop.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show noprint">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#example">
   Example
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#from-scratch">
   From scratch
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#tensorflow-tutorial-notebook">
   Tensorflow tutorial notebook
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Word2Vec Workshop</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#example">
   Example
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#from-scratch">
   From scratch
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#tensorflow-tutorial-notebook">
   Tensorflow tutorial notebook
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            
              <div>
                
  <div class="tex2jax_ignore mathjax_ignore section" id="word2vec-workshop">
<h1>Word2Vec Workshop<a class="headerlink" href="#word2vec-workshop" title="Permalink to this headline">¶</a></h1>
<div class="section" id="example">
<h2>Example<a class="headerlink" href="#example" title="Permalink to this headline">¶</a></h2>
<p>The following example is from <a class="reference external" href="https://iksinc.online/tag/skip-gram-model/">here</a>.
</p>
<p>Consider the training corpus having the following sentences:</p>
<p>“the dog saw a cat”,</p>
<p>“the dog chased the cat”,</p>
<p>“the cat climbed a tree”</p>
<p>In this corpus <span class="math notranslate nohighlight">\(V=8\)</span> and we are interested in creating word embeddings of order <span class="math notranslate nohighlight">\(d=3\)</span>. The parameter matrices are randomly initialized as</p>
<div class="math notranslate nohighlight">
\[\begin{split}W = \begin{bmatrix} 0.54 &amp;  0.28 &amp;  0.42\\\\0.84 &amp;  0.00 &amp;  0.12\\\\ 0.67 &amp;  0.83 &amp;  0.14\\\\0.58 &amp;  0.89 &amp;  0.21\\\\0.19 &amp;  0.11 &amp;  0.22\\\\0.98 &amp;  0.81 &amp;  0.17\\\\0.82 &amp;  0.27 &amp;  0.43\\\\0.94 &amp;  0.82 &amp;  0.34
\end{bmatrix}\end{split}\]</div>
<div class="math notranslate nohighlight">
\[\begin{split} W^\prime = \begin{bmatrix}0.18 &amp;  0.37 &amp;  0.01 &amp;  0.25 &amp;  0.80 &amp;  0.02 &amp;  0.60 &amp;  0.60\\\\0.11 &amp;  0.38 &amp;  0.04 &amp;  0.89 &amp;  0.98 &amp;  0.06 &amp;  0.89 &amp;  0.58\\\\0.74 &amp;  0.63 &amp;  0.58 &amp;  0.02 &amp;  0.21 &amp;  0.54 &amp;  0.77 &amp;  0.25
\end{bmatrix}\end{split}\]</div>
<p><strong>Suppose we want the network to learn relationship between the words “cat” and “climbed”. That is, the network should show a high probability for “cat” when “climbed” is presented at the input of the network.</strong> In word embedding terminology, the word “cat” is referred as the target word and the word “climbed” is referred as the context word. In this case, the input vector <span class="math notranslate nohighlight">\(\mathbf x_{t+1}  =  [0 0 0 1 0 0 0 0]^T\)</span>. Notice that only the 4th component of the vector is 1 - the input word “climbed” holds for example the 4th position in a sorted list of corpus words. Given that the target word is “cat”, the target vector will be <span class="math notranslate nohighlight">\(\mathbf x_t = [0 1 0 0 0 0 0 0 ]^T\)</span>.</p>
<p>With the input vector representing “climbed”, the output at the hidden layer neurons can be computed as</p>
<div class="math notranslate nohighlight">
\[\mathbf z_t^T = \mathbf x_{t+j}^T \mathbf W = \begin{bmatrix}
  0.58 &amp;  0.89 &amp;  0.21
\end{bmatrix}, j=1\]</div>
<p>Carrying out similar manipulations for hidden to output layer, the activation vector for output layer neurons can be written as</p>
<div class="math notranslate nohighlight">
\[\mathbf z^\prime_j = \mathbf z_t^T \mathbf W^\prime =\begin{bmatrix}
  0.35 &amp;  0.69 &amp;  0.16 &amp;  0.94 &amp;  1.38 &amp;  0.18 &amp;  1.30 &amp;  0.91
\end{bmatrix}\]</div>
<p>Since the goal is produce probabilities for words in the output layer,  <span class="math notranslate nohighlight">\(p(w_{t+j} | w_t; \theta)\)</span> to reflect their next word relationship with the context word at input, we need the sum of neuron outputs in the output layer to add to one. This can be achieved with the softmax</p>
<div class="math notranslate nohighlight">
\[\hat{\mathbf y}_j = \mathtt{softmax}(\mathbf z^\prime_j), j=1\]</div>
<p>Thus, the probabilities for eight words in the corpus are:</p>
<div class="math notranslate nohighlight">
\[\hat{\mathbf y} = \begin{bmatrix}
  0.35 &amp;  \mathbf{0.69} &amp;  0.16 &amp;  0.94 &amp;  1.38 &amp;  0.18 &amp;  1.30 &amp;  0.91
\end{bmatrix}\]</div>
<p>The probability in bold is for the chosen target word ‘cat’. Given the target vector [0 1 0 0 0 0 0 0 ], the error vector for the output layer is easily computed via CE loss. Once the loss is known, the weights in the matrices W can be updated using backpropagation. Thus, the training can proceed by presenting different context-target words pairs from the corpus. The context can be more than one word and in this case the loss is the average loss across all pairs.</p>
</div>
<div class="section" id="from-scratch">
<h2>From scratch<a class="headerlink" href="#from-scratch" title="Permalink to this headline">¶</a></h2>
<p>This <a class="reference external" href="https://nathanrooy.github.io/posts/2018-03-22/word2vec-from-scratch-with-python-and-numpy">self-contained implementation</a> is instructive and you should <a class="reference external" href="https://github.com/nathanrooy/word2vec-from-scratch-with-python/blob/master/word2vec.py">go through it</a> to understand the word2vec embedding.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1">#   Nathan A. Rooy</span>
<span class="c1">#   Simple word2vec from scratch with Python</span>
<span class="c1">#   2018-FEB</span>
<span class="c1">#</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">re</span>
<span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">defaultdict</span>



<span class="k">class</span> <span class="nc">word2vec</span><span class="p">():</span>
    <span class="k">def</span> <span class="fm">__init__</span> <span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n</span> <span class="o">=</span> <span class="n">settings</span><span class="p">[</span><span class="s1">&#39;n&#39;</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">eta</span> <span class="o">=</span> <span class="n">settings</span><span class="p">[</span><span class="s1">&#39;learning_rate&#39;</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">epochs</span> <span class="o">=</span> <span class="n">settings</span><span class="p">[</span><span class="s1">&#39;epochs&#39;</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">window</span> <span class="o">=</span> <span class="n">settings</span><span class="p">[</span><span class="s1">&#39;window_size&#39;</span><span class="p">]</span>
        <span class="k">pass</span>
    
    
    <span class="c1"># GENERATE TRAINING DATA</span>
    <span class="k">def</span> <span class="nf">generate_training_data</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">settings</span><span class="p">,</span> <span class="n">corpus</span><span class="p">):</span>

        <span class="c1"># GENERATE WORD COUNTS</span>
        <span class="n">word_counts</span> <span class="o">=</span> <span class="n">defaultdict</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">row</span> <span class="ow">in</span> <span class="n">corpus</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">row</span><span class="p">:</span>
                <span class="n">word_counts</span><span class="p">[</span><span class="n">word</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">v_count</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">word_counts</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>

        <span class="c1"># GENERATE LOOKUP DICTIONARIES</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">words_list</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">word_counts</span><span class="o">.</span><span class="n">keys</span><span class="p">()),</span><span class="n">reverse</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">word_index</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">((</span><span class="n">word</span><span class="p">,</span> <span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">word</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">words_list</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">index_word</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">((</span><span class="n">i</span><span class="p">,</span> <span class="n">word</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">word</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">words_list</span><span class="p">))</span>

        <span class="n">training_data</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="c1"># CYCLE THROUGH EACH SENTENCE IN CORPUS</span>
        <span class="k">for</span> <span class="n">sentence</span> <span class="ow">in</span> <span class="n">corpus</span><span class="p">:</span>
            <span class="n">sent_len</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">sentence</span><span class="p">)</span>

            <span class="c1"># CYCLE THROUGH EACH WORD IN SENTENCE</span>
            <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">word</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">sentence</span><span class="p">):</span>
                
                <span class="c1">#w_target  = sentence[i]</span>
                <span class="n">w_target</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">word2onehot</span><span class="p">(</span><span class="n">sentence</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>

                <span class="c1"># CYCLE THROUGH CONTEXT WINDOW</span>
                <span class="n">w_context</span> <span class="o">=</span> <span class="p">[]</span>
                <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">i</span><span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">window</span><span class="p">,</span> <span class="n">i</span><span class="o">+</span><span class="bp">self</span><span class="o">.</span><span class="n">window</span><span class="o">+</span><span class="mi">1</span><span class="p">):</span>
                    <span class="k">if</span> <span class="n">j</span><span class="o">!=</span><span class="n">i</span> <span class="ow">and</span> <span class="n">j</span><span class="o">&lt;=</span><span class="n">sent_len</span><span class="o">-</span><span class="mi">1</span> <span class="ow">and</span> <span class="n">j</span><span class="o">&gt;=</span><span class="mi">0</span><span class="p">:</span>
                        <span class="n">w_context</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">word2onehot</span><span class="p">(</span><span class="n">sentence</span><span class="p">[</span><span class="n">j</span><span class="p">]))</span>
                <span class="n">training_data</span><span class="o">.</span><span class="n">append</span><span class="p">([</span><span class="n">w_target</span><span class="p">,</span> <span class="n">w_context</span><span class="p">])</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">training_data</span><span class="p">)</span>


    <span class="c1"># SOFTMAX ACTIVATION FUNCTION</span>
    <span class="k">def</span> <span class="nf">softmax</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">e_x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">e_x</span> <span class="o">/</span> <span class="n">e_x</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>


    <span class="c1"># CONVERT WORD TO ONE HOT ENCODING</span>
    <span class="k">def</span> <span class="nf">word2onehot</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">word</span><span class="p">):</span>
        <span class="n">word_vec</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">v_count</span><span class="p">)]</span>
        <span class="n">word_index</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">word_index</span><span class="p">[</span><span class="n">word</span><span class="p">]</span>
        <span class="n">word_vec</span><span class="p">[</span><span class="n">word_index</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="k">return</span> <span class="n">word_vec</span>


    <span class="c1"># FORWARD PASS</span>
    <span class="k">def</span> <span class="nf">forward_pass</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">h</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">w1</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
        <span class="n">u</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">w2</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">h</span><span class="p">)</span>
        <span class="n">y_c</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">u</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">y_c</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">u</span>
                

    <span class="c1"># BACKPROPAGATION</span>
    <span class="k">def</span> <span class="nf">backprop</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">e</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">dl_dw2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">outer</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">e</span><span class="p">)</span>  
        <span class="n">dl_dw1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">outer</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">w2</span><span class="p">,</span> <span class="n">e</span><span class="o">.</span><span class="n">T</span><span class="p">))</span>

        <span class="c1"># UPDATE WEIGHTS</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">w1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">w1</span> <span class="o">-</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">eta</span> <span class="o">*</span> <span class="n">dl_dw1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">w2</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">w2</span> <span class="o">-</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">eta</span> <span class="o">*</span> <span class="n">dl_dw2</span><span class="p">)</span>
        <span class="k">pass</span>


    <span class="c1"># TRAIN W2V model</span>
    <span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">training_data</span><span class="p">):</span>
        <span class="c1"># INITIALIZE WEIGHT MATRICES</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">w1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="mf">0.8</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">v_count</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n</span><span class="p">))</span>     <span class="c1"># embedding matrix</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">w2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="mf">0.8</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">v_count</span><span class="p">))</span>     <span class="c1"># context matrix</span>
        
        <span class="c1"># CYCLE THROUGH EACH EPOCH</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">epochs</span><span class="p">):</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">loss</span> <span class="o">=</span> <span class="mi">0</span>

            <span class="c1"># CYCLE THROUGH EACH TRAINING SAMPLE</span>
            <span class="k">for</span> <span class="n">w_t</span><span class="p">,</span> <span class="n">w_c</span> <span class="ow">in</span> <span class="n">training_data</span><span class="p">:</span>

                <span class="c1"># FORWARD PASS</span>
                <span class="n">y_pred</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">u</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward_pass</span><span class="p">(</span><span class="n">w_t</span><span class="p">)</span>
                
                <span class="c1"># CALCULATE ERROR</span>
                <span class="n">EI</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">subtract</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">word</span><span class="p">)</span> <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">w_c</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

                <span class="c1"># BACKPROPAGATION</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">backprop</span><span class="p">(</span><span class="n">EI</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">w_t</span><span class="p">)</span>

                <span class="c1"># CALCULATE LOSS</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">loss</span> <span class="o">+=</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">([</span><span class="n">u</span><span class="p">[</span><span class="n">word</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="mi">1</span><span class="p">)]</span> <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">w_c</span><span class="p">])</span> <span class="o">+</span> <span class="nb">len</span><span class="p">(</span><span class="n">w_c</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">u</span><span class="p">)))</span>
                <span class="c1">#self.loss += -2*np.log(len(w_c)) -np.sum([u[word.index(1)] for word in w_c]) + (len(w_c) * np.log(np.sum(np.exp(u))))</span>
                
            <span class="nb">print</span> <span class="s1">&#39;EPOCH:&#39;</span><span class="p">,</span><span class="n">i</span><span class="p">,</span> <span class="s1">&#39;LOSS:&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss</span>
        <span class="k">pass</span>


    <span class="c1"># input a word, returns a vector (if available)</span>
    <span class="k">def</span> <span class="nf">word_vec</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">word</span><span class="p">):</span>
        <span class="n">w_index</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">word_index</span><span class="p">[</span><span class="n">word</span><span class="p">]</span>
        <span class="n">v_w</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">w1</span><span class="p">[</span><span class="n">w_index</span><span class="p">]</span>
        <span class="k">return</span> <span class="n">v_w</span>


    <span class="c1"># input a vector, returns nearest word(s)</span>
    <span class="k">def</span> <span class="nf">vec_sim</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">vec</span><span class="p">,</span> <span class="n">top_n</span><span class="p">):</span>

        <span class="c1"># CYCLE THROUGH VOCAB</span>
        <span class="n">word_sim</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">v_count</span><span class="p">):</span>
            <span class="n">v_w2</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">w1</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
            <span class="n">theta_num</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">vec</span><span class="p">,</span> <span class="n">v_w2</span><span class="p">)</span>
            <span class="n">theta_den</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">vec</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">v_w2</span><span class="p">)</span>
            <span class="n">theta</span> <span class="o">=</span> <span class="n">theta_num</span> <span class="o">/</span> <span class="n">theta_den</span>

            <span class="n">word</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">index_word</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
            <span class="n">word_sim</span><span class="p">[</span><span class="n">word</span><span class="p">]</span> <span class="o">=</span> <span class="n">theta</span>

        <span class="n">words_sorted</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">word_sim</span><span class="o">.</span><span class="n">items</span><span class="p">(),</span> <span class="n">key</span><span class="o">=</span><span class="k">lambda</span><span class="p">(</span><span class="n">word</span><span class="p">,</span> <span class="n">sim</span><span class="p">):</span><span class="n">sim</span><span class="p">,</span> <span class="n">reverse</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">word</span><span class="p">,</span> <span class="n">sim</span> <span class="ow">in</span> <span class="n">words_sorted</span><span class="p">[:</span><span class="n">top_n</span><span class="p">]:</span>
            <span class="nb">print</span> <span class="n">word</span><span class="p">,</span> <span class="n">sim</span>
            
        <span class="k">pass</span>

    <span class="c1"># input word, returns top [n] most similar words</span>
    <span class="k">def</span> <span class="nf">word_sim</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">word</span><span class="p">,</span> <span class="n">top_n</span><span class="p">):</span>
        
        <span class="n">w1_index</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">word_index</span><span class="p">[</span><span class="n">word</span><span class="p">]</span>
        <span class="n">v_w1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">w1</span><span class="p">[</span><span class="n">w1_index</span><span class="p">]</span>

        <span class="c1"># CYCLE THROUGH VOCAB</span>
        <span class="n">word_sim</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">v_count</span><span class="p">):</span>
            <span class="n">v_w2</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">w1</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
            <span class="n">theta_num</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">v_w1</span><span class="p">,</span> <span class="n">v_w2</span><span class="p">)</span>
            <span class="n">theta_den</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">v_w1</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">v_w2</span><span class="p">)</span>
            <span class="n">theta</span> <span class="o">=</span> <span class="n">theta_num</span> <span class="o">/</span> <span class="n">theta_den</span>

            <span class="n">word</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">index_word</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
            <span class="n">word_sim</span><span class="p">[</span><span class="n">word</span><span class="p">]</span> <span class="o">=</span> <span class="n">theta</span>

        <span class="n">words_sorted</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">word_sim</span><span class="o">.</span><span class="n">items</span><span class="p">(),</span> <span class="n">key</span><span class="o">=</span><span class="k">lambda</span><span class="p">(</span><span class="n">word</span><span class="p">,</span> <span class="n">sim</span><span class="p">):</span><span class="n">sim</span><span class="p">,</span> <span class="n">reverse</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">word</span><span class="p">,</span> <span class="n">sim</span> <span class="ow">in</span> <span class="n">words_sorted</span><span class="p">[:</span><span class="n">top_n</span><span class="p">]:</span>
            <span class="nb">print</span> <span class="n">word</span><span class="p">,</span> <span class="n">sim</span>
            
        <span class="k">pass</span>

<span class="c1">#--- EXAMPLE RUN --------------------------------------------------------------+</span>

<span class="n">settings</span> <span class="o">=</span> <span class="p">{}</span>
<span class="n">settings</span><span class="p">[</span><span class="s1">&#39;n&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">5</span>                   <span class="c1"># dimension of word embeddings</span>
<span class="n">settings</span><span class="p">[</span><span class="s1">&#39;window_size&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">2</span>         <span class="c1"># context window +/- center word</span>
<span class="n">settings</span><span class="p">[</span><span class="s1">&#39;min_count&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>           <span class="c1"># minimum word count</span>
<span class="n">settings</span><span class="p">[</span><span class="s1">&#39;epochs&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">5000</span>           <span class="c1"># number of training epochs</span>
<span class="n">settings</span><span class="p">[</span><span class="s1">&#39;neg_samp&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">10</span>           <span class="c1"># number of negative words to use during training</span>
<span class="n">settings</span><span class="p">[</span><span class="s1">&#39;learning_rate&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.01</span>    <span class="c1"># learning rate</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>                   <span class="c1"># set the seed for reproducibility</span>

<span class="n">corpus</span> <span class="o">=</span> <span class="p">[[</span><span class="s1">&#39;the&#39;</span><span class="p">,</span><span class="s1">&#39;quick&#39;</span><span class="p">,</span><span class="s1">&#39;brown&#39;</span><span class="p">,</span><span class="s1">&#39;fox&#39;</span><span class="p">,</span><span class="s1">&#39;jumped&#39;</span><span class="p">,</span><span class="s1">&#39;over&#39;</span><span class="p">,</span><span class="s1">&#39;the&#39;</span><span class="p">,</span><span class="s1">&#39;lazy&#39;</span><span class="p">,</span><span class="s1">&#39;dog&#39;</span><span class="p">]]</span>

<span class="c1"># INITIALIZE W2V MODEL</span>
<span class="n">w2v</span> <span class="o">=</span> <span class="n">word2vec</span><span class="p">()</span>

<span class="c1"># generate training data</span>
<span class="n">training_data</span> <span class="o">=</span> <span class="n">w2v</span><span class="o">.</span><span class="n">generate_training_data</span><span class="p">(</span><span class="n">settings</span><span class="p">,</span> <span class="n">corpus</span><span class="p">)</span>

<span class="c1"># train word2vec model</span>
<span class="n">w2v</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">training_data</span><span class="p">)</span>

<span class="c1">#--- END ----------------------------------------------------------------------+</span>
</pre></div>
</div>
</div>
<div class="section" id="tensorflow-tutorial-notebook">
<h2>Tensorflow tutorial notebook<a class="headerlink" href="#tensorflow-tutorial-notebook" title="Permalink to this headline">¶</a></h2>
<iframe src="https://nbviewer.jupyter.org/github/tensorflow/docs/blob/master/site/en/tutorials/text/word2vec.ipynb" width="900" height="1200"></iframe>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./aiml-common/lectures/nlp/word2vec"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            
                <!-- Previous / next buttons -->
<div class='prev-next-area'>
</div>
            
        </div>
    </div>
    <footer class="footer">
  <p>
    
      By Pantelis Monogioudis, Ph.D<br/>
    
        &copy; Copyright 2021.<br/>
  </p>
</footer>
</main>


      </div>
    </div>
  
  <script src="../../../../_static/js/index.be7d3bbb2ef33a8344ce.js"></script>

  </body>
</html>